## -*- coding: UTF-8 -*-
# @author: Copilot
# @date: 2026.01
# Description: Design, Creativity and Hardware News Crawler

#!/usr/bin/python3

import requests
from bs4 import BeautifulSoup
import os
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import encrypt_and_verify_url
import mysqlWriteNewsV2
import scrapers_design as scrapers
from email.utils import formataddr
import ssl
import json
import re
import difflib
from sklearn.feature_extraction.text import TfidfVectorizer

# è¯»å–é…ç½®
config_path = os.path.join(os.path.dirname(__file__), ".", "tech_key_config_map.json")
if os.path.exists(config_path):
    with open(config_path) as cfg_f:
        cfg = json.load(cfg_f)
else:
    cfg = {}

KEYWORDS_RANK_MAP = cfg.get("KEYWORDS_RANK_MAP", {})
# ä¸ºè®¾è®¡/ç¡¬ä»¶å¢åŠ ä¸€äº›é»˜è®¤æƒé‡ï¼Œå¦‚æœé…ç½®ä¸­æ²¡æœ‰
design_defaults = {
    "design": 0.8,
    "award": 0.9,
    "creative": 0.8,
    "style": 0.7,
    "concept": 0.9,
    "supercar": 1.0,
    "hypercar": 1.0,
    "industrial": 0.7,
    "architecture": 0.6,
    "çº¢ç‚¹": 1.2,
    "iFè®¾è®¡": 1.2,
    "å·¥ä¸šè®¾è®¡": 1.0,
    "é€ å‹": 0.8,
    "åˆ›æ„": 0.8,
    "è¶…è·‘": 1.2,
    "å¥¢å": 0.8,
    "å¥¢ä¾ˆå“": 0.9,
    "ç¡¬ä»¶": 0.7,
    "æ±½è½¦": 0.6,
    "æ——èˆ°": 0.7,
    "robot": 1.2,
    "robotics": 1.0,
    "drone": 0.8,
    "3D printing": 0.9,
    "wearable": 0.7,
    "vehicle": 0.6,
    "automotive": 0.7,
    "smartphone": 0.5,
    "laptop": 0.5,
    "camera": 0.6,
    "flight": 0.8,
    "drone": 0.7,
    "ev": 0.9,
    "electric vehicle": 0.9,
    "autonomous": 0.8,
}
for k, v in design_defaults.items():
    if k not in KEYWORDS_RANK_MAP:
        KEYWORDS_RANK_MAP[k] = v

BLOCKED_DOMAINS = cfg.get("BLOCKED_DOMAINS", [])
kRankLevelValue = cfg.get("RANK_THRESHOLD", 1.0)  # è®¾è®¡ç±»é˜ˆå€¼ç¨å¾®è°ƒä½ä¸€ç‚¹ï¼Œå› ä¸ºå…³é”®è¯å¯èƒ½æ²¡é‚£ä¹ˆå¯†é›†

stop_words = set(
    [
        "is",
        "the",
        "this",
        "and",
        "a",
        "to",
        "of",
        "in",
        "for",
        "on",
        "if",
        "has",
        "are",
        "was",
        "be",
        "by",
        "at",
        "that",
        "it",
        "its",
        "as",
        "about",
        "an",
        "or",
        "but",
        "not",
        "from",
        "with",
        "which",
        "there",
        "when",
        "so",
        "all",
        "any",
        "some",
        "one",
        "two",
    ]
)


class NewsScraper:
    """é€šç”¨æ–°é—»çˆ¬è™«åŸºç±»"""

    def __init__(self, source_name):
        self.source_name = source_name
        self.articles = []

    def calculate_weight(self, title):
        """è®¡ç®—æ–°é—»æƒé‡"""
        return self.compute_rank_from_map(
            title, KEYWORDS_RANK_MAP, fuzzy=True, threshold=0.7
        )

    def determine_category(self, title, source_name):
        """æ ¹æ®æ ‡é¢˜å’Œæ¥æºç¡®å®šæ–‡ç« åˆ†ç±»"""
        title_lower = title.lower()

        # ç¡¬ä»¶/æ±½è½¦ åˆ†ç±»å…³é”®è¯
        hardware_words = [
            "car",
            "automotive",
            "vehicle",
            "concept",
            "hardware",
            "device",
            "gadget",
            "supercar",
            "hypercar",
            "watch",
            "luxury",
            "engine",
            "motor",
            "smartphone",
            "laptop",
            "camera",
            "è¶…è·‘",
            "å¥¢ä¾ˆå“",
            "ç¡¬ä»¶",
            "æ±½è½¦",
            "è‡ªåŠ¨é©¾é©¶",
            "æ‘©æ‰˜",
            "åº§é©¾",
            "æ——èˆ°",
            "æ‰‹æœº",
            "ç”µè„‘",
            "ç›¸æœº",
            "æ˜¾å¡",
            "EV",
            "electric vehicle",
            "hyper-car",
            "luxury watch",
            "precision",
            "craftsmanship",
        ]

        # å¦‚æœæ¥æºæœ¬èº«å°±æ˜¯æ±½è½¦ç±»
        if source_name == "Car Design News":
            return "ç¡¬ä»¶/æ±½è½¦"

        if any(word in title_lower for word in hardware_words):
            return "ç¡¬ä»¶/æ±½è½¦"

        # é»˜è®¤å½’ç±»ä¸º è®¾è®¡/åˆ›æ„
        return "è®¾è®¡/åˆ›æ„"

    def filter_and_store(self, keywords="è®¾è®¡"):
        """è¿‡æ»¤å¹¶å­˜å‚¨ç¬¦åˆæ¡ä»¶çš„æ–‡ç« """
        filtered_articles = []
        current_date = datetime.now()
        six_months_ago = current_date - timedelta(days=180)

        for title, url, weight, image_url in self.articles:
            if weight > kRankLevelValue:
                # è¿‡æ»¤è¿‡é•¿çš„URLå’Œæ ‡é¢˜ï¼Œé˜²æ­¢æ•°æ®åº“æŠ¥é”™ (æ•°æ®åº“å­—æ®µé™åˆ¶ä¸º255)
                if len(url) > 250:
                    print(f"âš ï¸ URLè¿‡é•¿ ({len(url)})ï¼Œè·³è¿‡: {url[:50]}...")
                    continue

                if len(title) > 250:
                    title = title[:245] + "..."

                # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
                db_publish_time_str = mysqlWriteNewsV2.getArticlePublishTime(url)

                if db_publish_time_str:
                    print(f"æ–‡ç« å·²å­˜åœ¨äºæ•°æ®åº“: {title[:30]}...")
                    continue

                # ç¡®å®šåˆ†ç±»
                category = self.determine_category(title, self.source_name)

                # å†™å…¥æ•°æ®åº“ (åŒ…å« image_url)
                publish_time = datetime.now().strftime("%Y-%m-%d_%H:%M")
                content = f"æ¥è‡ª {self.source_name} çš„{category}å†…å®¹"

                newsOne = (
                    weight,
                    title,
                    self.source_name,
                    publish_time,
                    content,
                    url,
                    keywords,
                    category,
                    image_url,
                )

                sql = """ INSERT INTO techTB(Rate,title,author,publish_time,content,url,key_word,category,image_url)
                          VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s) """

                result = mysqlWriteNewsV2.writeDb(sql, newsOne)
                if result:
                    print(f"âœ… æˆåŠŸå†™å…¥ [{category}]: {title[:50]}...")
                    filtered_articles.append((title, url, weight))
                else:
                    print(f"âŒ å†™å…¥å¤±è´¥: {title[:50]}...")

        return filtered_articles

    def compute_rank_from_map(self, text, key_map, fuzzy=False, threshold=0.8):
        if not text or not key_map:
            return 0.0
        text_lower = re.sub(r"\W+", " ", text).lower()
        tokens = text_lower.split()
        rank = 0.0
        for key, weight in key_map.items():
            if not isinstance(key, str):
                continue
            key_l = key.lower()
            if key_l in text_lower:
                rank += float(weight)
            elif fuzzy:
                # ç®€å•çš„æ¨¡ç³ŠåŒ¹é…å¢å¼º
                if len(key_l) > 3 and key_l[:4] in text_lower:
                    rank += float(weight) * 0.5
        return float(rank)


class GenericDesignWrapper(NewsScraper):
    def __init__(self, scraper_class, source_name):
        super().__init__(source_name)
        self.scraper = scraper_class()

    def scrape(self, limit=15):
        try:
            print(f"\nå¼€å§‹æŠ“å– {self.source_name}...")
            articles = self.scraper.scrape_articles(limit=limit)
            new_stored_articles = []
            for art in articles:
                weight = self.calculate_weight(art["title"])
                # ç»™è®¾è®¡ç±»æ¥æºä¸€å®šçš„åŸºç¡€åˆ†
                weight += 0.5
                self.articles.append(
                    (art["title"], art["url"], weight, art.get("image_url", ""))
                )

            # è¿”å›è¿‡æ»¤åçš„æ–‡ç« åˆ—è¡¨ï¼Œç”¨äºæ±‡æ€»
            return self.filter_and_store(self.source_name)
        except Exception as e:
            print(f"{self.source_name} æŠ“å–å¼‚å¸¸: {e}")
            return []


class DesignNewsAggregator:
    """è®¾è®¡æ–°é—»èšåˆä¸è¾“å‡ºç±»"""

    def __init__(self, filename="design_news_summary.txt"):
        self.filename = filename

    def save_to_txt(self, articles):
        """å°†æ–°é—»ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶"""
        with open(self.filename, "w", encoding="utf-8") as f:
            f.write(f"æ¯æ—¥è®¾è®¡ä¸åˆ›æ„æ–°é—»æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d')}\n")
            f.write(f"å…±æ”¶é›†åˆ° {len(articles)} ç¯‡é«˜ä»·å€¼æ–‡ç« \n")
            f.write("=" * 60 + "\n\n")

            for idx, (title, url, weight) in enumerate(articles, 1):
                f.write(f"{idx}. [{weight:.2f}] {title}\n")
                f.write(f"   ğŸ”— {url}\n\n")

        print(f"ğŸ’¾ æ–°é—»å·²ä¿å­˜åˆ° {os.path.abspath(self.filename)}")
        return self.filename


def send_news_email(txt_file, recipient):
    """å‘é€åŒ…å«æ–°é—»æ‘˜è¦çš„é‚®ä»¶"""
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    import smtplib

    try:
        # è§£å¯†è·å–æˆæƒç 
        _pwd = encrypt_and_verify_url.decrypt_getKey(
            "dm1wbmFmYmxsdnR0YmJlaQ==".encode("utf-8")
        )

        # è¯»å–å†…å®¹
        with open(txt_file, "r", encoding="utf-8") as f:
            news_content = f.read()

        sender = "840056598@qq.com"
        subject = f"æ¯æ—¥è®¾è®¡ä¸åˆ›æ„æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d')}"

        msg = MIMEMultipart()
        msg["From"] = sender
        msg["To"] = formataddr(["äº²çˆ±çš„è®¾è®¡çˆ±å¥½è€…", recipient])
        msg["Subject"] = subject
        msg.attach(MIMEText(news_content, "plain", "utf-8"))

        server = smtplib.SMTP_SSL("smtp.qq.com", 465)
        server.login(sender, _pwd.decode("utf-8"))
        server.sendmail(sender, [recipient], msg.as_string())
        server.quit()

        print(f"ğŸ“§ é‚®ä»¶å·²æˆåŠŸå‘é€è‡³ {recipient}")
        return True
    except Exception as e:
        print(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
        return False


def main():
    print(f"=== è®¾è®¡ä¸åˆ›æ„æ–°é—»é‡‡é›†ä»»åŠ¡å¼€å§‹: {datetime.now()} ===")

    scrapers_to_run = [
        (scrapers.RedDotScraper, "Red Dot Award"),
        (scrapers.IFDesignScraper, "iF Design Award"),
        (scrapers.DesignboomScraper, "Designboom"),
        (scrapers.PopularMechanicsScraper, "Popular Mechanics"),
        (scrapers.DezeenScraper, "Dezeen"),
        (scrapers.MakezineScraper, "Makezine"),
        (scrapers.InfoQProductScraper, "InfoQ Product"),
        (scrapers.ProductHuntScraper, "Product Hunt"),
        (scrapers.YankoDesignScraper, "Yanko Design"),
        (scrapers.CoreDesignScraper, "Core77"),
        (scrapers.CarDesignNewsScraper, "Car Design News"),
        (scrapers.LVMHScraper, "LVMH"),
        (scrapers.KeringScraper, "Kering"),
        (scrapers.CarBuzzScraper, "CarBuzz"),
        (scrapers.CarAndDriverScraper, "Car and Driver"),
    ]

    all_articles = []
    total_count = 0
    for scraper_cls, name in scrapers_to_run:
        wrapper = GenericDesignWrapper(scraper_cls, name)
        results = wrapper.scrape(limit=12)
        all_articles.extend(results)
        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(2)

    print(f"\n=== é‡‡é›†ä»»åŠ¡ç»“æŸ. å…±æ–°å¢ {len(all_articles)} æ¡é«˜ä»·å€¼è®°å½• ===")

    if all_articles:
        # æŒ‰æƒé‡æ’åº
        all_articles.sort(key=lambda x: x[2], reverse=True)

        aggregator = DesignNewsAggregator()
        txt_file = aggregator.save_to_txt(all_articles)

        # å‘é€é‚®ä»¶
        send_news_email(txt_file, "840056598@qq.com")
    else:
        print("ä»Šæ—¥æ— æ–°å¢é«˜ä»·å€¼è®¾è®¡èµ„è®¯ã€‚")


if __name__ == "__main__":
    main()
