## -*- coding: UTF-8 -*-
# @author: Copilot
# @date: 2026.01
# Description: Economic and Enterprise News Crawler

#!/usr/bin/python3

import requests
from bs4 import BeautifulSoup
import os
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import encrypt_and_verify_url
import mysqlWriteNewsV2
import scrapers_economy as scrapers
from email.utils import formataddr
import ssl
import json
import re
import difflib
from sklearn.feature_extraction.text import TfidfVectorizer

# è¯»å–é…ç½®
with open(
    os.path.join(os.path.dirname(__file__), ".", "economy_key_config_map.json")
) as cfg_f:
    cfg = json.load(cfg_f)
KEYWORDS_RANK_MAP = cfg.get("KEYWORDS_RANK_MAP", {})
BLOCKED_DOMAINS = cfg.get("BLOCKED_DOMAINS", [])
CUSTOM_SITES = cfg.get("CUSTOM_SITES", [])
# æ˜¯å¦åœ¨å¿…è¦æ—¶è¯·æ±‚è¯¦æƒ…é¡µé¢ä»¥æå–å‘å¸ƒæ—¶é—´
FETCH_DETAILS_FOR_DATE = cfg.get("FETCH_DETAILS_FOR_DATE", False)

# å…¨å±€é…ç½®
OUTPUT_FILE = "economy_news_summary.txt"
# æ–°é—»ä»·å€¼é˜ˆå€¼
kRankLevelValue = cfg.get("RANK_THRESHOLD", 0.5)

# Define a set of common words to filter out (stop words)
stop_words = set(
    [
        "is",
        "the",
        "this",
        "and",
        "a",
        "to",
        "of",
        "in",
        "for",
        "on",
        "if",
        "has",
        "are",
        "was",
        "be",
        "by",
        "at",
        "that",
        "it",
        "its",
        "as",
        "about",
        "an",
        "or",
        "but",
        "not",
        "from",
        "with",
        "which",
        "there",
        "when",
        "so",
        "all",
        "any",
        "some",
        "one",
        "two",
        "three",
        "four",
        "five",
    ]
)


class NewsScraper:
    """é€šç”¨æ–°é—»çˆ¬è™«åŸºç±»"""

    def __init__(self, source_name):
        self.source_name = source_name
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            }
        )
        self.base_url = ""
        self.articles = []

    def scrape(self):
        """å­ç±»éœ€å®ç°çš„å…·ä½“çˆ¬å–é€»è¾‘"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° scrape æ–¹æ³•")

    def get_random_delay(self):
        """éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°"""
        time.sleep(random.uniform(1, 3))

    def calculate_weight(self, title):
        """è®¡ç®—æ–°é—»æƒé‡"""
        return self.calculate_keyword_weights([title], KEYWORDS_RANK_MAP)

    def determine_category(self, title, keywords):
        """æ ¹æ®æ ‡é¢˜å’Œå…³é”®è¯ç¡®å®šæ–‡ç« åˆ†ç±»"""
        title_lower = title.lower()

        # å®šä¹‰ç»æµåˆ†ç±»è§„åˆ™
        categories = {
            "å®è§‚ç»æµ": [
                "gdp",
                "inflation",
                "fed",
                "imf",
                "policy",
                "rate",
                "tax",
                "economy",
                "macro",
            ],
            "è‚¡å¸‚æŠ•èµ„": [
                "stock",
                "market",
                "nasdaq",
                "dow",
                "ipo",
                "earnings",
                "fund",
                "invest",
            ],
            "ä¼ä¸šåŠ¨æ€": [
                "merger",
                "acquisition",
                "ceo",
                "layoff",
                "report",
                "microsoft",
                "apple",
                "tesla",
                "google",
                "amazon",
            ],
            "é‡‘èç§‘æŠ€": ["fintech", "crypto", "bitcoin", "blockchain", "bank", "payment"],
            "è´¸æ˜“ä¸ä¾›åº”é“¾": [
                "trade",
                "tariff",
                "supply chain",
                "freight",
                "export",
                "import",
            ],
            "ç»æµè¯„è®º": ["opinion", "analysis", "forecast", "outlook", "trend"],
        }

        for cat, words in categories.items():
            if any(word in title_lower for word in words):
                return cat
        return "ç»æµç»¼åˆ"

    def filter_and_store(self, keywords="ç»æµ"):
        """è¿‡æ»¤å¹¶å­˜å‚¨ç¬¦åˆæ¡ä»¶çš„æ–‡ç« """
        filtered_articles = []
        current_date = datetime.now()
        six_months_ago = current_date - timedelta(days=180)

        for title, url, weight in self.articles:
            if weight > kRankLevelValue:
                # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
                db_publish_time_str = mysqlWriteNewsV2.getArticlePublishTime(url)

                if db_publish_time_str:
                    try:
                        # å°è¯•è§£ææ•°æ®åº“ä¸­çš„æ—¶é—´æ ¼å¼ %Y-%m-%d_%H:%M
                        db_publish_time = datetime.strptime(
                            db_publish_time_str, "%Y-%m-%d_%H:%M"
                        )
                        if db_publish_time < six_months_ago:
                            print(f"æ–‡ç« å·²è¶…è¿‡åŠå¹´ï¼Œä¸å†å‘é€: {url}")
                            continue
                        else:
                            print(f"æ–‡ç« å·²åœ¨æ•°æ®åº“ä½†æœªæ»¡åŠå¹´ï¼Œç»§ç»­å‘é€: {title[:50]}...")
                            filtered_articles.append((title, url, weight))
                            continue
                    except Exception as e:
                        print(f"è§£ææ•°æ®åº“æ—¶é—´å¤±è´¥: {e}, é»˜è®¤è·³è¿‡")
                        continue

                filtered_articles.append((title, url, weight))

                # ç¡®å®šåˆ†ç±»
                category = self.determine_category(title, keywords)

                # å†™å…¥æ•°æ®åº“
                publish_time = datetime.now().strftime("%Y-%m-%d_%H:%M")
                newsOne = (
                    weight,
                    title,
                    self.source_name,
                    publish_time,
                    "content",
                    url,
                    keywords,
                    category,
                )
                sql = """ INSERT INTO techTB(Rate,title,author,publish_time,content,url,key_word,category) VALUES(%s,%s,%s,%s,%s,%s,%s,%s) """
                result = mysqlWriteNewsV2.writeDb(sql, newsOne)
                if result:
                    print(f"âœ… æˆåŠŸå†™å…¥æ•°æ®åº“ [{category}]: {title[:50]}...")
                else:
                    print(f"âŒ å†™å…¥æ•°æ®åº“å¤±è´¥: {title[:50]}...")

        return filtered_articles

    def compute_rank_from_map(self, text, key_map, fuzzy=False, threshold=0.8):
        if not text or not key_map:
            return 0.0
        text_lower = re.sub(r"\W+", " ", text).lower()
        tokens = text_lower.split()
        rank = 0.0
        for key, weight in key_map.items():
            try:
                key_l = key.lower()
            except Exception:
                continue
            if key_l in text_lower:
                rank += float(weight)
            elif fuzzy:
                seq = difflib.SequenceMatcher(None, key_l, text_lower)
                if seq.ratio() >= threshold:
                    rank += float(weight) * seq.ratio()
                else:
                    for t in tokens:
                        seq2 = difflib.SequenceMatcher(None, key_l, t)
                        if seq2.ratio() >= threshold:
                            rank += float(weight) * seq2.ratio()
                            break
        return float(rank)

    # è®¡ç®—å…³é”®è¯æƒé‡
    def calculate_keyword_weights(self, texts, keywords):
        try:
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform(texts)
            feature_names = vectorizer.get_feature_names_out()
        except:
            # Fallback if TF-IDF fails (e.g. empty texts)
            text = " ".join(texts)
            return self.compute_rank_from_map(text, keywords, fuzzy=True, threshold=0.7)

        text = " ".join(texts)
        return self.compute_rank_from_map(text, keywords, fuzzy=True, threshold=0.7)


class BloombergWrapper(NewsScraper):
    def __init__(self):
        super().__init__("Bloomberg")
        self.scraper = scrapers.BloombergScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("Bloomberg")
        except Exception as e:
            print(f"Bloomberg çˆ¬å–å¤±è´¥: {e}")
            return []


class CNBCWrapper(NewsScraper):
    def __init__(self):
        super().__init__("CNBC")
        self.scraper = scrapers.CNBCScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("CNBC")
        except Exception as e:
            print(f"CNBC çˆ¬å–å¤±è´¥: {e}")
            return []


class EconomistWrapper(NewsScraper):
    def __init__(self):
        super().__init__("The Economist")
        self.scraper = scrapers.EconomistScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("The Economist")
        except Exception as e:
            print(f"The Economist çˆ¬å–å¤±è´¥: {e}")
            return []


class GartnerWrapper(NewsScraper):
    def __init__(self):
        super().__init__("Gartner")
        self.scraper = scrapers.GartnerScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("Gartner")
        except Exception as e:
            print(f"Gartner çˆ¬å–å¤±è´¥: {e}")
            return []


class SinaWrapper(NewsScraper):
    def __init__(self):
        super().__init__("Sina Finance")
        self.scraper = scrapers.SinaFinanceScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("æ–°æµªè´¢ç»")
        except Exception as e:
            print(f"æ–°æµªè´¢ç» çˆ¬å–å¤±è´¥: {e}")
            return []


class CaixinWrapper(NewsScraper):
    def __init__(self):
        super().__init__("è´¢æ–°ç½‘")
        self.scraper = scrapers.CaixinScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("è´¢æ–°ç½‘")
        except Exception as e:
            print(f"è´¢æ–°ç½‘ çˆ¬å–å¤±è´¥: {e}")
            return []


class WallStreetCNWrapper(NewsScraper):
    def __init__(self):
        super().__init__("ä¸œæ–¹è´¢å¯Œç½‘")
        self.scraper = scrapers.WallStreetCNScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("ä¸œæ–¹è´¢å¯Œç½‘")
        except Exception as e:
            print(f"ä¸œæ–¹è´¢å¯Œç½‘ çˆ¬å–å¤±è´¥: {e}")
            return []


class TencentStockWrapper(NewsScraper):
    def __init__(self):
        super().__init__("æ–°æµªè´¢ç»")
        self.scraper = scrapers.TencentStockScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("æ–°æµªè´¢ç»")
        except Exception as e:
            print(f"æ–°æµªè´¢ç» çˆ¬å–å¤±è´¥: {e}")
            return []


class EconomyNewsAggregator:
    """ç»æµæ–°é—»èšåˆå™¨"""

    def __init__(self):
        self.scrapers = [
            BloombergWrapper(),
            CNBCWrapper(),
            EconomistWrapper(),
            GartnerWrapper(),
            SinaWrapper(),
            CaixinWrapper(),
            WallStreetCNWrapper(),
            TencentStockWrapper(),
        ]

    def collect_news(self):
        """æ”¶é›†æ‰€æœ‰æ¥æºçš„æ–°é—»"""
        all_articles = []

        print(f"ğŸ“… å¼€å§‹æ”¶é›†ç»æµæ–°é—» - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ” ä»·å€¼é˜ˆå€¼: > {kRankLevelValue}")
        print("-" * 60)

        for scraper in self.scrapers:
            print(f"\nğŸ“¡ æ­£åœ¨é‡‡é›† {scraper.source_name}...")
            try:
                articles = scraper.scrape()
                print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æœ‰ä»·å€¼æ–‡ç« ")
                all_articles.extend(articles)
            except Exception as e:
                print(f"âŒ é‡‡é›†å¤±è´¥: {str(e)}")

        # æŒ‰æƒé‡æ’åº
        all_articles.sort(key=lambda x: x[2], reverse=True)
        return all_articles

    def save_to_txt(self, articles, filename=OUTPUT_FILE):
        """å°†æ–°é—»ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶"""
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"æ¯æ—¥ç»æµæ–°é—»æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d')}\n")
            f.write(f"å…±æ”¶é›†åˆ° {len(articles)} ç¯‡é«˜ä»·å€¼æ–‡ç« \n")
            f.write("=" * 60 + "\n\n")

            for idx, (title, url, weight) in enumerate(articles, 1):
                f.write(f"{idx}. [{weight:.2f}] {title}\n")
                f.write(f"   ğŸ”— {url}\n\n")

        print(f"ğŸ’¾ æ–°é—»å·²ä¿å­˜åˆ° {os.path.abspath(filename)}")
        return filename


def send_news_email(txt_file, recipient):
    """å‘é€åŒ…å«æ–°é—»æ‘˜è¦çš„é‚®ä»¶"""
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    import smtplib

    try:
        _pwd = encrypt_and_verify_url.decrypt_getKey(
            "dm1wbmFmYmxsdnR0YmJlaQ==".encode("utf-8")
        )
    except:
        print("Decrypt key failed, skipping email.")
        return

    # è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹
    with open(txt_file, "r", encoding="utf-8") as f:
        news_content = f.read()

    # é‚®ä»¶é…ç½®
    sender = "840056598@qq.com"
    subject = f"æ¯æ—¥ç»æµæ–°é—»æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d')}"

    # åˆ›å»ºé‚®ä»¶
    msg = MIMEMultipart()
    msg["From"] = sender
    msg["To"] = formataddr(["äº²çˆ±çš„ç”¨æˆ·", recipient])
    msg["Subject"] = subject

    # æ·»åŠ æ–‡æœ¬å†…å®¹
    msg.attach(MIMEText(news_content, "plain", "utf-8"))

    # å‘é€é‚®ä»¶
    try:
        server = smtplib.SMTP_SSL("smtp.qq.com", 465)
        server.login(sender, _pwd.decode("utf-8"))
        server.sendmail(
            sender,
            [
                recipient,
            ],
            msg.as_string(),
        )
        print(f"ğŸ“§ é‚®ä»¶å·²æˆåŠŸå‘é€è‡³ {recipient}")
        server.quit()
        return True
    except Exception as e:
        print(f"âŒ å‘é€å¤±è´¥: {str(e)}")


# ä¸»æ‰§è¡Œæµç¨‹
if __name__ == "__main__":
    # åˆ›å»ºèšåˆå™¨å¹¶æ”¶é›†æ–°é—»
    aggregator = EconomyNewsAggregator()
    articles = aggregator.collect_news()

    # ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
    txt_file = aggregator.save_to_txt(articles)

    # å‘é€é‚®ä»¶
    send_news_email(txt_file, "840056598@qq.com")
