## -*- coding: UTF-8 -*-
#@author: JACK YANG
#@date:
      # 2022.09 add rank map
      # 2024.10 scikit-learn
      # 2025.07 scraperç±»çˆ¬è™«
# @Email: yyjqr789@sina.com

#!/usr/bin/python3

import requests
from bs4 import BeautifulSoup
import os
from datetime import datetime
from typing import List, Dict, Optional

import encrypt_and_verify_url
from email.utils import formataddr
import ssl
import json

from sklearn.feature_extraction.text import TfidfVectorizer


# å…¨å±€é…ç½®
OUTPUT_FILE = "tech_news_summary.txt"
KEYWORDS_RANK_MAP = {...}  # æ‚¨çš„å…³é”®è¯æƒé‡æ˜ å°„
kRankLevelValue = 0.5  # æ–°é—»ä»·å€¼é˜ˆå€¼
# Define a set of common words to filter out (stop words)
stop_words = set([
    "is", "the", "this", "and", "a", "to", "of", "in", "for", "on",
    "if", "has", "are", "was", "be", "by", "at", "that", "it", "its",
    "as","about",
    "an", "or", "but", "not", "from", "with", "which", "there", "when",
    "so", "all", "any", "some", "one", "two", "three", "four", "five"
])



class NewsScraper:
    """é€šç”¨æ–°é—»çˆ¬è™«åŸºç±»"""
    def __init__(self, source_name):
        self.source_name = source_name
        self.session = requests.Session()  # æ·»åŠ å…±äº«çš„ Session å¯¹è±¡
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        })
        self.base_url = ""
        self.articles = []   
    def scrape(self):
        """å­ç±»éœ€å®ç°çš„å…·ä½“çˆ¬å–é€»è¾‘"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° scrape æ–¹æ³•")
    
    def filter_and_store(self):
        """è¿‡æ»¤å¹¶å­˜å‚¨ç¬¦åˆæ¡ä»¶çš„æ–‡ç« """
        filtered_articles = []
        for title, url, weight in self.articles:
            if weight > kRankLevelValue:
                filtered_articles.append((title, url, weight))
        return filtered_articles
    # è®¡ç®—å…³é”®è¯æƒé‡
    def calculate_keyword_weights(self, texts, keywords):
        vectorizer = TfidfVectorizer()

        tfidf_matrix = vectorizer.fit_transform(texts)
    ##scikit-learn>1.0.x use this version
        feature_names = vectorizer.get_feature_names_out()
        print("test feature_names", feature_names)
        # Define a set of common words to filter out (stop words)
    # Filter out stop words and numbers from feature names
        filtered_feature_names = [feature for feature in feature_names if feature not in stop_words and not feature.isdigit()]
        # Assuming we want to select the top N important feature names
    # For demonstration, let's say we want the top 3 features
    # You can replace this logic with your own importance criteria
        top_n = 6
        important_feature_names = filtered_feature_names[:top_n]  # Select top N feature names
        print("important_feature_names:{0}".format(important_feature_names));
        keyword_indices = []
        keyword_weights_sum = 0
        for keyword in keywords:
            if keyword in filtered_feature_names:
                index = filtered_feature_names.index(keyword)
                keyword_indices.append(index)
                keyword_weights = tfidf_matrix[:, index].toarray()
                print("Keyword: {0}, Index: {1}, Weight: {2}".format(keyword, index, keyword_weights))
                keyword_weights_sum += keyword_weights.sum()
        return keyword_weights_sum


class MitScraper(NewsScraper):
    """MIT Technology Review çˆ¬è™«"""
    def __init__(self):
        super().__init__("MIT Technology Review")
    
    def scrape(self):
        url = 'https://www.technologyreview.com/'
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        
        news_elements = soup.find_all(class_='homepageStoryCard__wrapper--5d95dc382241d259dc249996a6e29782')
        print(f"weight test")   
        for news_element in news_elements:
            try:
                title_elem = news_element.find(class_='homepageStoryCard__hed--92c78a74bbc694463e43e32aafbbdfd7')
                link_elem = news_element.find('a')
                
                if title_elem and link_elem:
                    title = title_elem.text.strip()
                    url = link_elem['href']
                    
                    # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                    if not url.startswith('http'):
                        url = f"https://www.technologyreview.com{url}"
                    
                    # è®¡ç®—æ–°é—»æƒé‡
                    weight = self.calculate_weight(title)
                    print(f"weight is {weight}")
                    self.articles.append((title, url, weight))
            except Exception as e:
                print(f"å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {str(e)}")
        
        return self.filter_and_store()
    
    def calculate_weight(self, title):
        """è®¡ç®—æ–°é—»æƒé‡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œä½¿ç”¨æ‚¨çš„å®é™…æƒé‡è®¡ç®—é€»è¾‘
        return self.calculate_keyword_weights([title], KEYWORDS_RANK_MAP)
        #return 0.3  # ç¤ºä¾‹å€¼

class HackerNewsScraper(NewsScraper):
    """Hacker News çˆ¬è™«"""
    def __init__(self):
        #super().__init__("Hacker News")
        super().__init__("Hacker News")
        self.base_url = "https://hacker-news.firebaseio.com/v0"
        self.source = "Hacker News"
    def scrape(self, limit: int = 10) -> List[Dict]:
        """çˆ¬å– Hacker News çƒ­é—¨æ–‡ç« """
        articles = []
        try:
            # è·å–çƒ­é—¨æ–‡ç« ID
            response = self.session.get(f"{self.base_url}/topstories.json")
            story_ids = response.json()[:limit]

            for story_id in story_ids:
                self.get_random_delay()

                # è·å–æ–‡ç« è¯¦æƒ…
                story_response = self.session.get(f"{self.base_url}/item/{story_id}.json")
                story_data = story_response.json()

                if story_data and story_data.get('url'):
                    article = {
                        'title': story_data.get('title', ''),
                        'url': story_data.get('url', ''),
                        'summary': f"Hacker Newsçƒ­é—¨æ–‡ç« ï¼Œå¾—åˆ†ï¼š{story_data.get('score', 0)}",
                        'source': self.source,
                        'author': story_data.get('by', ''),
                        'tags': 'Tech,News',
                        'publish_time': datetime.fromtimestamp(story_data.get('time', 0)),
                        'views': story_data.get('score', 0),
                        'likes': story_data.get('descendants', 0)
                    }
                     # è®¡ç®—æ–°é—»æƒé‡
                    weight = self.calculate_weight(title)
                    self.articles.append(article,url,weight)

            print(f"æˆåŠŸçˆ¬å– {len(articles)} ç¯‡ Hacker News æ–‡ç« ")
            return self.filter_and_store()

        except Exception as e:
            print(f"çˆ¬å– Hacker News å¤±è´¥: {e}")
            return []


class TechNewsAggregator:
    """ç§‘æŠ€æ–°é—»èšåˆå™¨"""
    def __init__(self):
        self.scrapers = [
            MitScraper(),
            #HackerNewsScraper(),
            # æ·»åŠ å…¶ä»–æ¥æºçš„çˆ¬è™«...
        ]
    
    def collect_news(self):
        """æ”¶é›†æ‰€æœ‰æ¥æºçš„æ–°é—»"""
        all_articles = []
        
        print(f"ğŸ“… å¼€å§‹æ”¶é›†ç§‘æŠ€æ–°é—» - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ” ä»·å€¼é˜ˆå€¼: > {kRankLevelValue}")
        print("-" * 60)
        
        for scraper in self.scrapers:
            print(f"\nğŸ“¡ æ­£åœ¨é‡‡é›† {scraper.source_name}...")
            try:
                articles = scraper.scrape()
                print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æœ‰ä»·å€¼æ–‡ç« ")
                all_articles.extend(articles)
            except Exception as e:
                print(f"âŒ é‡‡é›†å¤±è´¥: {str(e)}")
        
        # æŒ‰æƒé‡æ’åº
        all_articles.sort(key=lambda x: x[2], reverse=True)
        return all_articles
    
    def save_to_txt(self, articles, filename=OUTPUT_FILE):
        """å°†æ–°é—»ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"æ¯æ—¥ç§‘æŠ€æ–°é—»æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d')}\n")
            f.write(f"å…±æ”¶é›†åˆ° {len(articles)} ç¯‡é«˜ä»·å€¼æ–‡ç« \n")
            f.write("=" * 60 + "\n\n")
            
            for idx, (title, url, weight) in enumerate(articles, 1):
                f.write(f"{idx}. [{weight:.2f}] {title}\n")
                f.write(f"   ğŸ”— {url}\n\n")
        
        print(f"ğŸ’¾ æ–°é—»å·²ä¿å­˜åˆ° {os.path.abspath(filename)}")
        return filename

def send_news_email(txt_file, recipient):
    """å‘é€åŒ…å«æ–°é—»æ‘˜è¦çš„é‚®ä»¶"""
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    import smtplib
    _pwd =encrypt_and_verify_url.decrypt_getKey("dm1wbmFmYmxsdnR0YmJlaQ==".encode("utf-8"))
    # è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹
    with open(txt_file, 'r', encoding='utf-8') as f:
        news_content = f.read()
    
    # é‚®ä»¶é…ç½®
    sender = "840056598@qq.com"
    password = _pwd
    subject = f"æ¯æ—¥ç§‘æŠ€æ–°é—»æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d')}"
    
    # åˆ›å»ºé‚®ä»¶
    msg = MIMEMultipart()
    msg['From'] = sender
    receiver = recipient 
    msg['To']=formataddr(["äº²çˆ±çš„ç”¨æˆ·",receiver])  #æ‹¬å·é‡Œçš„å¯¹åº”æ”¶ä»¶äººé‚®ç®±
    msg['Subject'] = subject
    
    # æ·»åŠ æ–‡æœ¬å†…å®¹
    msg.attach(MIMEText(news_content, 'plain', 'utf-8'))
    print(f"send subject {subject}")
    # åˆ›å»ºå®‰å…¨ä¸Šä¸‹æ–‡ï¼ˆè§£å†³SSLéªŒè¯é—®é¢˜ï¼‰
    context = ssl.create_default_context()
    context.check_hostname = False
    context.verify_mode = ssl.CERT_NONE
    # å‘é€é‚®ä»¶
    try:
        #with smtplib.SMTP_SSL('smtp.qq.com', 465, context) as server:
            #server.login(sender, _pwd.decode("utf-8"))
            #print("login email OK\n")
            #server.sendmail(sender, [receiver,], msg.as_string())
        server=smtplib.SMTP_SSL("smtp.qq.com",465) #å‘ä»¶äººé‚®ç®±ä¸­çš„SMTPæœåŠ¡å™¨ï¼Œç«¯å£æ˜¯25 (é»˜è®¤ï¼‰---------->465
        server.login(sender,_pwd.decode("utf-8"))  #æ‹¬å·ä¸­å¯¹åº”çš„æ˜¯å‘ä»¶äººé‚®ç®±è´¦å·ã€é‚®ç®±å¯†ç 
        server.sendmail(sender,[receiver,],msg.as_string())  #æ‹¬å·ä¸­å¯¹åº”çš„æ˜¯å‘ä»¶äººé‚®ç®±è´¦å·ã€æ”¶ä»¶äººé‚®ç®±è´¦å·ã€å‘é€é‚®ä»¶
        print(f"ğŸ“§ é‚®ä»¶å·²æˆåŠŸå‘é€è‡³ {recipient}")
        print ('SEND NEWS AND IMG OK')
        server.quit()  #è¿™å¥æ˜¯å…³é—­è¿æ¥
        return True
    except smtplib.SMTPAuthenticationError:
        print("âŒ è®¤è¯å¤±è´¥: è¯·æ£€æŸ¥é‚®ç®±å’Œæˆæƒç æ˜¯å¦æ­£ç¡®")
        print("ğŸ’¡ æç¤º: QQé‚®ç®±éœ€è¦ä½¿ç”¨æˆæƒç è€Œéå¯†ç ")
    except smtplib.SMTPException as e:
        print(f"âŒ SMTPåè®®é”™è¯¯: {str(e)}")
        print(f"é”™è¯¯ä»£ç : {e.smtp_code}")
        print(f"é”™è¯¯æ¶ˆæ¯: {e.smtp_error.decode('utf-8')}")
    except Exception as e:
        print(f"âŒ å‘é€å¤±è´¥: {str(e)}")
# ä¸»æ‰§è¡Œæµç¨‹
if __name__ == "__main__":
    with open('./tech_key_config_map.json') as j:

       KEYWORDS_RANK_MAP=json.load(j)['KEYWORDS_RANK_MAP']

    # åˆ›å»ºèšåˆå™¨å¹¶æ”¶é›†æ–°é—»
    aggregator = TechNewsAggregator()
    articles = aggregator.collect_news()
    
    # ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
    txt_file = aggregator.save_to_txt(articles)
    
    # å‘é€é‚®ä»¶
    send_news_email(txt_file, "840056598@qq.com")
    
    # å¯é€‰ï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶
    # os.remove(txt_file)
