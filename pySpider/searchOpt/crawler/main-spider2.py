## -*- coding: UTF-8 -*-
#@author: JACK YANG
#@date:
      # 2022.09 add rank map
      # 2024.10 scikit-learn
      # 2025.07 scraperç±»çˆ¬è™«
# @Email: yyjqr789@sina.com

#!/usr/bin/python3

import requests
import argparse
import sys
from bs4 import BeautifulSoup
import os
from datetime import datetime
from typing import List, Dict, Optional

import encrypt_and_verify_url
from email.utils import formataddr
import ssl
import json
import re
import difflib
import time
try:
    # Silence insecure request warnings when fallback verify=False is used
    from urllib3.exceptions import InsecureRequestWarning
    requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
except Exception:
    pass

#from sklearn.feature_extraction.text import TfidfVectorizer

# è¯»å–é…ç½®
with open(os.path.join(os.path.dirname(__file__), '.', 'tech_key_config_map.json')) as cfg_f:
    cfg = json.load(cfg_f)
KEYWORDS_RANK_MAP = cfg.get('KEYWORDS_RANK_MAP', {})
BLOCKED_DOMAINS = cfg.get('BLOCKED_DOMAINS', [])
CUSTOM_SITES = cfg.get('CUSTOM_SITES', [])
# æ˜¯å¦åœ¨å¿…è¦æ—¶è¯·æ±‚è¯¦æƒ…é¡µé¢ä»¥æå–å‘å¸ƒæ—¶é—´ï¼ˆé»˜è®¤ Falseï¼Œé¿å…å¤§é‡é¢å¤–è¯·æ±‚ï¼‰
FETCH_DETAILS_FOR_DATE = cfg.get('FETCH_DETAILS_FOR_DATE', False)

# å…¨å±€é…ç½®
OUTPUT_FILE = "tech_news_summary.txt"
# é»˜è®¤é˜ˆå€¼ï¼Œå¯åœ¨é…ç½®ä¸­è¦†ç›–
kRankLevelValue = cfg.get('RANK_THRESHOLD', 0.5)
# Define a set of common words to filter out (stop words)
stop_words = set([
    "is", "the", "this", "and", "a", "to", "of", "in", "for", "on",
    "if", "has", "are", "was", "be", "by", "at", "that", "it", "its",
    "as","about",
    "an", "or", "but", "not", "from", "with", "which", "there", "when",
    "so", "all", "any", "some", "one", "two", "three", "four", "five"
])



class NewsScraper:
    """é€šç”¨æ–°é—»çˆ¬è™«åŸºç±»"""
    def __init__(self, source_name):
        self.source_name = source_name
        self.session = requests.Session()  # æ·»åŠ å…±äº«çš„ Session å¯¹è±¡
        # æ·»åŠ é‡è¯•ç­–ç•¥ä»¥æå‡åµŒå…¥å¼è®¾å¤‡æˆ–ä¸ç¨³å®šç½‘ç»œä¸‹çš„å¥å£®æ€§
        try:
            from requests.adapters import HTTPAdapter
            from urllib3.util.retry import Retry
            retry = Retry(total=3, backoff_factor=0.3, status_forcelist=(500,502,503,504))
            adapter = HTTPAdapter(max_retries=retry)
            self.session.mount('http://', adapter)
            self.session.mount('https://', adapter)
        except Exception:
            pass
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        })
        self.base_url = ""
        self.articles = []
    def scrape(self):
        """å­ç±»éœ€å®ç°çš„å…·ä½“çˆ¬å–é€»è¾‘"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° scrape æ–¹æ³•")
    
    def filter_and_store(self):
        """è¿‡æ»¤å¹¶å­˜å‚¨ç¬¦åˆæ¡ä»¶çš„æ–‡ç« """
        filtered_articles = []
        for item in self.articles:
            # æ”¯æŒ (title,url,weight) å’Œ (title,url,weight,date)
            if not item or len(item) < 3:
                continue
            title = item[0]
            url = item[1]
            weight = item[2]
            date = item[3] if len(item) > 3 else None
            if weight > kRankLevelValue:
                filtered_articles.append((title, url, weight, date))
        return filtered_articles
    # è®¡ç®—å…³é”®è¯æƒé‡
    def calculate_keyword_weights(self, texts, keywords):
        # ä½¿ç”¨è½»é‡æ¨¡ç³ŠåŒ¹é…è®¡ç®—æƒé‡ï¼Œé¿å…æ¯æ¬¡æ„å»º TF-IDF
        text = ' '.join(texts)
        return compute_rank_from_map(text, keywords, fuzzy=True, threshold=0.7)


def compute_rank_from_map(text, key_map, fuzzy=False, threshold=0.8):
    if not text or not key_map:
        return 0.0
    text_lower = re.sub(r'\W+', ' ', text).lower()
    tokens = text_lower.split()
    rank = 0.0
    for key, weight in key_map.items():
        try:
            key_l = key.lower()
        except Exception:
            continue
        if key_l in text_lower:
            rank += float(weight)
        elif fuzzy:
            seq = difflib.SequenceMatcher(None, key_l, text_lower)
            if seq.ratio() >= threshold:
                rank += float(weight) * seq.ratio()
            else:
                for t in tokens:
                    seq2 = difflib.SequenceMatcher(None, key_l, t)
                    if seq2.ratio() >= threshold:
                        rank += float(weight) * seq2.ratio()
                        break
    return float(rank)


def _parse_monthname_date(text: str) -> Optional[str]:
    # æ”¯æŒåƒ "September 8, 2025" æˆ– "Sep 8, 2025"
    months = {
        'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'sept':9,'oct':10,'nov':11,'dec':12
    }
    m = re.search(r'([A-Za-z]{3,9})\s+(\d{1,2}),?\s*(20\d{2})', text)
    if m:
        mon = m.group(1).lower()[:3]
        d = int(m.group(2))
        y = int(m.group(3))
        monn = months.get(mon)
        if monn:
            try:
                return f"{y:04d}-{monn:02d}-{d:02d}"
            except Exception:
                return None
    return None


def extract_date_from_url_or_title(url: str, title: str, session: requests.Session = None, fetch_details: bool = False) -> Optional[str]:
    # 1) ä» URL ä¸­åŒ¹é… /YYYY/MM/DD/ æˆ– /YYYY/MM/DD- æˆ– /YYYY/MM-DD ç­‰å¸¸è§æ ¼å¼
    try:
        if url:
            m = re.search(r'/([12]\d{3})[/-](\d{1,2})[/-](\d{1,2})/', url)
            if not m:
                # æœ‰äº› URL å½¢å¼ä¸º /YYYY/MM/DD/slug æˆ– /YYYY/MM/DD
                m = re.search(r'/([12]\d{3})/([01]?\d)/([0-3]?\d)(?:/|$)', url)
            if m:
                y = int(m.group(1))
                mo = int(m.group(2))
                d = int(m.group(3))
                return f"{y:04d}-{mo:02d}-{d:02d}"
    except Exception:
        pass

    # 2) ä»æ ‡é¢˜ä¸­è§£æ MonthName DD, YYYY
    if title:
        dt = _parse_monthname_date(title)
        if dt:
            return dt
        m2 = re.search(r'(20\d{2})', title)
        if m2:
            # åªæ‰¾åˆ°å¹´ä»½ï¼Œè¿”å›å¹´ä»½å ä½
            return f"{m2.group(1)}"

    # 3) å¯é€‰ï¼šè¯·æ±‚è¯¦æƒ…é¡µå¹¶ä» meta æ ‡ç­¾æˆ– time æ ‡ç­¾ä¸­è¯»å–
    if fetch_details and session and url:
        try:
            resp = session.get(url, timeout=6)
            if resp and getattr(resp, 'status_code', None) == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')
                # common meta tags
                meta_props = [
                    ('property','article:published_time'),
                    ('name','article:published_time'),
                    ('name','pubdate'),
                    ('name','publishdate'),
                    ('name','publish_date'),
                    ('name','date'),
                    ('itemprop','datePublished')
                ]
                for attr, val in meta_props:
                    tag = soup.find('meta', attrs={attr: val})
                    if tag and tag.get('content'):
                        cont = tag.get('content')
                        m = re.search(r'(20\d{2})[-/](\d{1,2})[-/](\d{1,2})', cont)
                        if m:
                            return f"{int(m.group(1)):04d}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"
                # æŸ¥æ‰¾ time æ ‡ç­¾
                t = soup.find('time')
                if t:
                    # ä¼˜å…ˆ datetime å±æ€§
                    dtstr = t.get('datetime') or t.get_text()
                    m = re.search(r'(20\d{2})[-/](\d{1,2})[-/](\d{1,2})', dtstr)
                    if m:
                        return f"{int(m.group(1)):04d}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"
        except Exception:
            pass

    return None


class MitScraper(NewsScraper):
    """MIT Technology Review çˆ¬è™«"""
    def __init__(self):
        super().__init__("MIT Technology Review")
    
    def scrape(self):
        url = 'https://www.technologyreview.com/'
        response = self.session.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        
        news_elements = soup.find_all(class_='homepageStoryCard__wrapper--5d95dc382241d259dc249996a6e29782')
        print(f"weight test")   
        for news_element in news_elements:
            try:
                title_elem = news_element.find(class_='homepageStoryCard__hed--92c78a74bbc694463e43e32aafbbdfd7')
                link_elem = news_element.find('a')
                
                if title_elem and link_elem:
                    title = title_elem.text.strip()
                    url = link_elem['href']
                    
                    # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                    if not url.startswith('http'):
                        url = f"https://www.technologyreview.com{url}"
                    
                    # å°è¯•ä» URL æˆ–æ ‡é¢˜ä¸­æå–å‘å¸ƒæ—¥æœŸï¼ˆæ— éœ€å†è¯·æ±‚è¯¦æƒ…é¡µï¼‰
                    pubdate = extract_date_from_url_or_title(url, title, session=self.session, fetch_details=FETCH_DETAILS_FOR_DATE)
                    # è®¡ç®—æ–°é—»æƒé‡
                    weight = self.calculate_weight(title)
                    if weight > 0 :
                        print(f"{url},  ###weight is: {weight:.2f}")
                    self.articles.append((title, url, weight, pubdate))
            except Exception as e:
                print(f"å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {str(e)}")
        
        return self.filter_and_store()
    
    def calculate_weight(self, title):
        """è®¡ç®—æ–°é—»æƒé‡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œä½¿ç”¨æ‚¨çš„å®é™…æƒé‡è®¡ç®—é€»è¾‘
        return self.calculate_keyword_weights([title], KEYWORDS_RANK_MAP)
        #return 0.3  # ç¤ºä¾‹å€¼

class HackerNewsScraper(NewsScraper):
    """Hacker News çˆ¬è™«"""
    def __init__(self):
        #super().__init__("Hacker News")
        super().__init__("Hacker News")
        self.base_url = "https://hacker-news.firebaseio.com/v0"
        self.source = "Hacker News"
    def scrape(self, limit: int = 10) -> List[Dict]:
        """çˆ¬å– Hacker News çƒ­é—¨æ–‡ç« """
        articles = []
        try:
            # è·å–çƒ­é—¨æ–‡ç« ID
            response = self.session.get(f"{self.base_url}/topstories.json")
            story_ids = response.json()[:limit]

            for story_id in story_ids:
                time.sleep(0.1)

                # è·å–æ–‡ç« è¯¦æƒ…
                story_response = self.session.get(f"{self.base_url}/item/{story_id}.json", timeout=6)
                story_data = story_response.json()

                if story_data and story_data.get('url'):
                    title = story_data.get('title', '')
                    url = story_data.get('url', '')
                    # å°è¯•ä» URL æˆ–æ ‡é¢˜ä¸­æå–å‘å¸ƒæ—¥æœŸ
                    pubdate = extract_date_from_url_or_title(url, title, session=self.session, fetch_details=FETCH_DETAILS_FOR_DATE)
                    # è®¡ç®—æ–°é—»æƒé‡
                    weight = self.calculate_keyword_weights([title], KEYWORDS_RANK_MAP)
                    self.articles.append((title, url, weight, pubdate))

            print(f"æˆåŠŸçˆ¬å– {len(self.articles)} ç¯‡ Hacker News æ–‡ç« ")
            return self.filter_and_store()

        except Exception as e:
            print(f"çˆ¬å– Hacker News å¤±è´¥: {e}")
            return []


class GitHubTrendingScraper(NewsScraper):
    """çˆ¬å– GitHub Trending é¡µé¢"""
    def __init__(self):
        super().__init__("GitHub Trending")

    def scrape(self):
        from urllib.parse import urljoin, urlparse
        url = 'https://github.com/trending'
        try:
            # é¦–å…ˆå°è¯•æ­£å¸¸è¯·æ±‚
            resp = self.session.get(url, timeout=8)
            if resp is None or getattr(resp, 'status_code', None) != 200:
                raise Exception(f"é200å“åº”: {getattr(resp, 'status_code', None)}")
            soup = BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            print(f"GitHubTrendingScraper åˆæ¬¡è¯·æ±‚å¤±è´¥: {e}")
            # åœ¨å—é™ç¯å¢ƒï¼ˆåµŒå…¥å¼è®¾å¤‡ï¼‰ä¸­ï¼Œæœ‰æ—¶éœ€è¦å¿½ç•¥è¯ä¹¦æˆ–ç¦ç”¨ hostname æ£€æŸ¥æ¥å°è¯•è·å–é¡µé¢ï¼ˆä»…ä½œé™çº§è¯Šæ–­ï¼‰
            try:
                resp = self.session.get(url, timeout=8, verify=False)
                if resp is None or getattr(resp, 'status_code', None) != 200:
                    raise Exception(f"é™çº§è¯·æ±‚é200: {getattr(resp, 'status_code', None)}")
                soup = BeautifulSoup(resp.text, 'html.parser')
                print("GitHubTrendingScraper: ä½¿ç”¨ verify=False æˆåŠŸè·å–é¡µé¢ï¼ˆé™çº§æ¨¡å¼ï¼‰")
            except Exception as e2:
                print(f"GitHubTrendingScraper é™çº§è¯·æ±‚ä¹Ÿå¤±è´¥: {e2}")
                # æœ€åä¸€æ‹›ï¼šå°è¯•é€šè¿‡ç¬¬ä¸‰æ–¹æ–‡æœ¬ä»£ç†æœåŠ¡è·å–é¡µé¢ï¼ˆä¾‹å¦‚ r.jina.aiï¼‰â€”â€”é€‚ç”¨äºæœ¬åœ° TLS/SNI é™åˆ¶åœºæ™¯
                try:
                    proxy_url = f"https://r.jina.ai/http://github.com/trending"
                    print(f"å°è¯•é€šè¿‡ä»£ç†æŠ“å–: {proxy_url}")
                    proxy_resp = self.session.get(proxy_url, timeout=10)
                    if proxy_resp is None or getattr(proxy_resp, 'status_code', None) != 200:
                        raise Exception(f"ä»£ç†é200: {getattr(proxy_resp, 'status_code', None)}")
                    soup = BeautifulSoup(proxy_resp.text, 'html.parser')
                    print("GitHubTrendingScraper: é€šè¿‡ä»£ç†æˆåŠŸè·å–é¡µé¢")
                except Exception as e3:
                    print(f"GitHubTrendingScraper ä»£ç†ä¹Ÿå¤±è´¥: {e3}")
                    return []

        # è§£æ trending é¡¹ç›®ï¼ˆå°½é‡ä¸é¢å¤–å‘èµ·è¯¦æƒ…é¡µè¯·æ±‚ï¼Œé™ä½ç½‘ç»œè´Ÿæ‹…ï¼‰
        items = soup.select('article.Box-row h1 a') or soup.select('h1.h3 a') or soup.select('article')
        for a in items:
            try:
                title = a.get_text(separator=' ', strip=True)
                href = a.get('href')
                if not href:
                    # æœ‰æ—¶ a æ˜¯å®¹å™¨ï¼Œéœ€è¦åœ¨å­å…ƒç´ æŸ¥æ‰¾
                    link_el = a.find('a') if hasattr(a, 'find') else None
                    href = link_el.get('href') if link_el is not None else None
                    if not href:
                        continue
                link = href if href.startswith('http') else urljoin('https://github.com', href)

                # è·³è¿‡è¢«é…ç½®ä¸ºé˜»æ–­çš„åŸŸå
                parsed = urlparse(link)
                domain = parsed.netloc.lower() if parsed.netloc else ''
                skip = False
                for bd in BLOCKED_DOMAINS:
                    if bd and bd in domain:
                        skip = True
                        break
                if skip:
                    # è·³è¿‡å·²çŸ¥å—é™åŸŸå
                    continue

                # åªä½¿ç”¨æ ‡é¢˜æ–‡æœ¬è¿›è¡Œæƒé‡è®¡ç®—ï¼ˆé¿å…è¯·æ±‚è¯¦æƒ…é¡µï¼‰
                pubdate = extract_date_from_url_or_title(link, title, session=self.session, fetch_details=FETCH_DETAILS_FOR_DATE)
                weight = self.calculate_keyword_weights([title], KEYWORDS_RANK_MAP)
                self.articles.append((title, link, weight, pubdate))
            except Exception as e:
                print(f"GitHubTrendingScraper è§£æå•é¡¹å¤±è´¥: {e}")

        return self.filter_and_store()


class DevToScraper(NewsScraper):
    """çˆ¬å– Dev.to çƒ­é—¨æ–‡ç« """
    def __init__(self):
        super().__init__("Dev.to")

    def scrape(self):
        url = 'https://dev.to/t/trending'
        try:
            resp = self.session.get(url, timeout=8)
            soup = BeautifulSoup(resp.text, 'html.parser')
            items = soup.select('h2 a, h3 a, a.crayons-story__hidden-navigation')
            for a in items:
                title = a.get_text(strip=True)
                href = a.get('href')
                if not href:
                    continue
                if href.startswith('http'):
                    link = href
                else:
                    link = 'https://dev.to' + href
                pubdate = extract_date_from_url_or_title(link, title, session=self.session, fetch_details=FETCH_DETAILS_FOR_DATE)
                weight = self.calculate_keyword_weights([title], KEYWORDS_RANK_MAP)
                self.articles.append((title, link, weight, pubdate))
        except Exception as e:
            print(f"DevToScraper å¤±è´¥: {e}")
        return self.filter_and_store()


class AITopicsScraper(NewsScraper):
    """çˆ¬å– AITopics æœç´¢ç»“æœ"""
    def __init__(self):
        super().__init__("AITopics")

    def scrape(self):
        url = 'https://aitopics.org/search'
        try:
            resp = self.session.get(url, timeout=8)
            soup = BeautifulSoup(resp.text, 'html.parser')
            items = soup.select('.searchtitle a')
            for a in items:
                title = a.get_text(strip=True)
                href = a.get('href')
                if not href:
                    continue
                if href.startswith('http'):
                    link = href
                else:
                    link = 'https://aitopics.org' + href
                pubdate = extract_date_from_url_or_title(link, title, session=self.session, fetch_details=FETCH_DETAILS_FOR_DATE)
                weight = self.calculate_keyword_weights([title], KEYWORDS_RANK_MAP)
                self.articles.append((title, link, weight, pubdate))
        except Exception as e:
            print(f"AITopicsScraper å¤±è´¥: {e}")
        return self.filter_and_store()


class GenericSiteScraper(NewsScraper):
    """åŸºäºé…ç½®çš„é€šç”¨ç«™ç‚¹çˆ¬è™«ï¼šæ”¯æŒ item/title/link çš„ CSS é€‰æ‹©å™¨

    é…ç½®å­—æ®µç¤ºä¾‹ (CUSTOM_SITES æ•°ç»„ä¸­çš„å¯¹è±¡):
      - name: åç§°
      - url: ç«™ç‚¹é¦–é¡µæˆ–åˆ—è¡¨é¡µ URL
      - item_selector: åˆ—è¡¨é¡¹ CSS é€‰æ‹©å™¨ (å¯é€‰)
      - title_selector: åœ¨ item å†…æå–æ ‡é¢˜çš„é€‰æ‹©å™¨ (å¯é€‰)
      - link_selector: åœ¨ item å†…æå–é“¾æ¥çš„é€‰æ‹©å™¨ (å¯é€‰)
      - rate_limit_ms: åœ¨è¯·æ±‚å‰ç­‰å¾…çš„æ¯«ç§’æ•° (å¯é€‰)
      - headers: å¯é€‰çš„ headers å¯¹è±¡ï¼Œä¼šä¸´æ—¶åˆå¹¶åˆ° session.headers
    """
    def __init__(self, source_name, site_cfg: dict):
        super().__init__(source_name)
        self.site_cfg = site_cfg

    def scrape(self):
        url = self.site_cfg.get('url')
        if not url:
            return []
        item_sel = self.site_cfg.get('item_selector')
        title_sel = self.site_cfg.get('title_selector')
        link_sel = self.site_cfg.get('link_selector')
        rate_limit_ms = int(self.site_cfg.get('rate_limit_ms', 0) or 0)
        site_headers = self.site_cfg.get('headers') or {}
        try:
            # ä¸´æ—¶åº”ç”¨ç«™ç‚¹ headers
            original_headers = dict(self.session.headers)
            if isinstance(site_headers, dict) and site_headers:
                self.session.headers.update(site_headers)

            # rate limit
            if rate_limit_ms > 0:
                time.sleep(rate_limit_ms / 1000.0)

            resp = self.session.get(url, timeout=10)
            # æ¢å¤åŸå§‹ headers
            self.session.headers.clear()
            self.session.headers.update(original_headers)

            soup = BeautifulSoup(resp.text, 'html.parser')
            if item_sel:
                items = soup.select(item_sel)
            else:
                items = soup.select('article')
            for it in items:
                title = None
                link = None
                if title_sel:
                    t = it.select_one(title_sel)
                    if t:
                        title = t.get_text(strip=True)
                if not title:
                    a = it.find('a')
                    if a:
                        title = a.get_text(strip=True)
                if link_sel:
                    l = it.select_one(link_sel)
                    if l and l.get('href'):
                        link = l.get('href')
                if not link:
                    a = it.find('a')
                    if a and a.get('href'):
                        link = a.get('href')
                if link and link.startswith('/'):
                    base = url.rstrip('/')
                    link = base + link
                if title and link:
                    pubdate = extract_date_from_url_or_title(link, title, session=self.session, fetch_details=FETCH_DETAILS_FOR_DATE)
                    weight = self.calculate_keyword_weights([title], KEYWORDS_RANK_MAP)
                    self.articles.append((title, link, weight, pubdate))
        except Exception as e:
            print(f"GenericSiteScraper({self.source_name}) å¤±è´¥: {e}")
        return self.filter_and_store()


class TechNewsAggregator:
    """ç§‘æŠ€æ–°é—»èšåˆå™¨"""
    def __init__(self):
        # å†…ç½®çˆ¬è™«
        self.scrapers = [
            MitScraper(),
            # ç¤¾åŒºå’Œå¹³å°
            HackerNewsScraper(),
            GitHubTrendingScraper() if 'GitHubTrendingScraper' in globals() else None,
            DevToScraper() if 'DevToScraper' in globals() else None,
            AITopicsScraper() if 'AITopicsScraper' in globals() else None,
        ]
        # ç§»é™¤ None
        self.scrapers = [s for s in self.scrapers if s]
        # åŠ è½½é…ç½®ä¸­è‡ªå®šä¹‰ç«™ç‚¹
        for site in CUSTOM_SITES:
            try:
                gs = GenericSiteScraper(site.get('name', 'custom'), site)
                self.scrapers.append(gs)
            except Exception as e:
                print(f"åŠ è½½è‡ªå®šä¹‰ç«™ç‚¹å¤±è´¥: {site} -> {e}")
    
    def collect_news(self):
        """æ”¶é›†æ‰€æœ‰æ¥æºçš„æ–°é—»"""
        all_articles = []
        
        print(f"ğŸ“… å¼€å§‹æ”¶é›†ç§‘æŠ€æ–°é—» - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ” ä»·å€¼é˜ˆå€¼: > {kRankLevelValue}")
        print("-" * 60)
        
        for scraper in self.scrapers:
            print(f"\nğŸ“¡ æ­£åœ¨é‡‡é›† {scraper.source_name}...")
            try:
                articles = scraper.scrape()
                print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æœ‰ä»·å€¼æ–‡ç« ")
                all_articles.extend(articles)
            except Exception as e:
                print(f"âŒ é‡‡é›†å¤±è´¥: {str(e)}")
        
        # æŒ‰æƒé‡æ’åº
        all_articles.sort(key=lambda x: x[2], reverse=True)
        return all_articles
    
    def save_to_txt(self, articles, filename=OUTPUT_FILE):
        """å°†æ–°é—»ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"æ¯æ—¥ç§‘æŠ€æ–°é—»æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d')}\n")
            f.write(f"å…±æ”¶é›†åˆ° {len(articles)} ç¯‡é«˜ä»·å€¼æ–‡ç« \n")
            f.write("=" * 60 + "\n\n")
            
            for idx, art in enumerate(articles, 1):
                # art is (title,url,weight,date)
                title = art[0]
                url = art[1]
                weight = art[2]
                date = art[3] if len(art) > 3 else None
                date_part = f" ({date})" if date else ""
                f.write(f"{idx}. [{weight:.2f}]{date_part} {title}\n")
                f.write(f"   ğŸ”— {url}\n\n")
        
        print(f"ğŸ’¾ æ–°é—»å·²ä¿å­˜åˆ° {os.path.abspath(filename)}")
        return filename

def send_news_email(txt_file, recipient):
    """å‘é€åŒ…å«æ–°é—»æ‘˜è¦çš„é‚®ä»¶"""
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    import smtplib
    _pwd =encrypt_and_verify_url.decrypt_getKey("dm1wbmFmYmxsdnR0YmJlaQ==".encode("utf-8"))
    # è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹
    with open(txt_file, 'r', encoding='utf-8') as f:
        news_content = f.read()
    
    # é‚®ä»¶é…ç½®
    sender = "840056598@qq.com"
    password = _pwd
    subject = f"æ¯æ—¥ç§‘æŠ€æ–°é—»æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d')}"
    
    # åˆ›å»ºé‚®ä»¶
    msg = MIMEMultipart()
    msg['From'] = sender
    receiver = recipient 
    msg['To']=formataddr(["äº²çˆ±çš„ç”¨æˆ·",receiver])  #æ‹¬å·é‡Œçš„å¯¹åº”æ”¶ä»¶äººé‚®ç®±
    msg['Subject'] = subject
    
    # æ·»åŠ æ–‡æœ¬å†…å®¹
    msg.attach(MIMEText(news_content, 'plain', 'utf-8'))
    print(f"send subject {subject}")
    # åˆ›å»ºå®‰å…¨ä¸Šä¸‹æ–‡ï¼ˆè§£å†³SSLéªŒè¯é—®é¢˜ï¼‰
    context = ssl.create_default_context()
    context.check_hostname = False
    context.verify_mode = ssl.CERT_NONE
    # å‘é€é‚®ä»¶
    try:
        #with smtplib.SMTP_SSL('smtp.qq.com', 465, context) as server:
            #server.login(sender, _pwd.decode("utf-8"))
            #print("login email OK\n")
            #server.sendmail(sender, [receiver,], msg.as_string())
        server=smtplib.SMTP_SSL("smtp.qq.com",465) #å‘ä»¶äººé‚®ç®±ä¸­çš„SMTPæœåŠ¡å™¨ï¼Œç«¯å£æ˜¯25 (é»˜è®¤ï¼‰---------->465
        server.login(sender,_pwd.decode("utf-8"))  #æ‹¬å·ä¸­å¯¹åº”çš„æ˜¯å‘ä»¶äººé‚®ç®±è´¦å·ã€é‚®ç®±å¯†ç 
        server.sendmail(sender,[receiver,],msg.as_string())  #æ‹¬å·ä¸­å¯¹åº”çš„æ˜¯å‘ä»¶äººé‚®ç®±è´¦å·ã€æ”¶ä»¶äººé‚®ç®±è´¦å·ã€å‘é€é‚®ä»¶
        print(f"ğŸ“§ é‚®ä»¶å·²æˆåŠŸå‘é€è‡³ {recipient}")
        print ('SEND NEWS AND IMG OK')
        server.quit()  #è¿™å¥æ˜¯å…³é—­è¿æ¥
        return True
    except smtplib.SMTPAuthenticationError:
        print("âŒ è®¤è¯å¤±è´¥: è¯·æ£€æŸ¥é‚®ç®±å’Œæˆæƒç æ˜¯å¦æ­£ç¡®")
        print("ğŸ’¡ æç¤º: QQé‚®ç®±éœ€è¦ä½¿ç”¨æˆæƒç è€Œéå¯†ç ")
    except smtplib.SMTPException as e:
        print(f"âŒ SMTPåè®®é”™è¯¯: {str(e)}")
        print(f"é”™è¯¯ä»£ç : {e.smtp_code}")
        print(f"é”™è¯¯æ¶ˆæ¯: {e.smtp_error.decode('utf-8')}")
    except Exception as e:
        print(f"âŒ å‘é€å¤±è´¥: {str(e)}")
# ä¸»æ‰§è¡Œæµç¨‹
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Tech news aggregator")
    parser.add_argument('--dry-run', action='store_true', help='åªæ”¶é›†å¹¶ä¿å­˜ç»“æœï¼Œè·³è¿‡å‘é€é‚®ä»¶')
    parser.add_argument('--github-only', action='store_true', help='ä»…è¿è¡Œ GitHub Trending æŠ“å–ï¼Œç”¨äºæµ‹è¯•')
    args = parser.parse_args()

    if args.github_only:
        print("ä»…è¿è¡Œ GitHub Trending æŠ“å–ï¼ˆdry-run æ¨¡å¼ä¸ä¼šå‘é€é‚®ä»¶ï¼‰")
        gs = GitHubTrendingScraper()
        articles = gs.scrape()
        # æ‰“å°ç»“æœå¹¶ä¿å­˜
        for idx, art in enumerate(articles, 1):
            title = art[0]
            url = art[1]
            weight = art[2]
            date = art[3] if len(art) > 3 else None
            date_part = f" ({date})" if date else ""
            print(f"{idx}. [{weight:.2f}]{date_part} {title}\n   {url}\n")
        out = TechNewsAggregator().save_to_txt(articles)
        if not args.dry_run:
            send_news_email(out, "840056598@qq.com")
        else:
            print("--dry-run: å·²è·³è¿‡å‘é€é‚®ä»¶")
        sys.exit(0)

    # åˆ›å»ºèšåˆå™¨å¹¶æ”¶é›†æ–°é—»
    aggregator = TechNewsAggregator()
    articles = aggregator.collect_news()

    # ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
    txt_file = aggregator.save_to_txt(articles)

    # å‘é€é‚®ä»¶ï¼ˆé™¤é dry-runï¼‰
    if args.dry_run:
        print("--dry-run: è·³è¿‡å‘é€é‚®ä»¶")
    else:
        send_news_email(txt_file, "840056598@qq.com")

    # å¯é€‰ï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶
    # os.remove(txt_file)
