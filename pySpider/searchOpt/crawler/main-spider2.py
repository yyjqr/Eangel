## -*- coding: UTF-8 -*-
# @author: JACK YANG
# @date:
# 2022.09 add rank map
# 2024.10 scikit-learn
# 2025.07 scraperç±»çˆ¬è™«
# 2026.01 å¢åŠ python Django å‰ç«¯å±•ç¤ºæ•°æ®åº“å†…å®¹
# @Email: yyjqr789@sina.com

#!/usr/bin/python3

import requests
from bs4 import BeautifulSoup
import os
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import encrypt_and_verify_url
import mysqlWriteNewsV2
import scrapers
from email.utils import formataddr
import ssl
import json
import re
import difflib
from sklearn.feature_extraction.text import TfidfVectorizer

# è¯»å–é…ç½®
with open(
    os.path.join(os.path.dirname(__file__), ".", "tech_key_config_map.json")
) as cfg_f:
    cfg = json.load(cfg_f)
KEYWORDS_RANK_MAP = cfg.get("KEYWORDS_RANK_MAP", {})
BLOCKED_DOMAINS = cfg.get("BLOCKED_DOMAINS", [])
CUSTOM_SITES = cfg.get("CUSTOM_SITES", [])
# æ˜¯å¦åœ¨å¿…è¦æ—¶è¯·æ±‚è¯¦æƒ…é¡µé¢ä»¥æå–å‘å¸ƒæ—¶é—´ï¼ˆé»˜è®¤ Falseï¼Œé¿å…å¤§é‡é¢å¤–è¯·æ±‚ï¼‰
FETCH_DETAILS_FOR_DATE = cfg.get("FETCH_DETAILS_FOR_DATE", False)

# å…¨å±€é…ç½®
OUTPUT_FILE = "tech_news_summary.txt"
# KEYWORDS_RANK_MAP = {...}  # æ‚¨çš„å…³é”®è¯æƒé‡æ˜ å°„
# æ–°é—»ä»·å€¼é˜ˆå€¼
kRankLevelValue = cfg.get("RANK_THRESHOLD", 0.5)
# Define a set of common words to filter out (stop words)
stop_words = set(
    [
        "is",
        "the",
        "this",
        "and",
        "a",
        "to",
        "of",
        "in",
        "for",
        "on",
        "if",
        "has",
        "are",
        "was",
        "be",
        "by",
        "at",
        "that",
        "it",
        "its",
        "as",
        "about",
        "an",
        "or",
        "but",
        "not",
        "from",
        "with",
        "which",
        "there",
        "when",
        "so",
        "all",
        "any",
        "some",
        "one",
        "two",
        "three",
        "four",
        "five",
    ]
)


class NewsScraper:
    """é€šç”¨æ–°é—»çˆ¬è™«åŸºç±»"""

    def __init__(self, source_name):
        self.source_name = source_name
        self.session = requests.Session()  # æ·»åŠ å…±äº«çš„ Session å¯¹è±¡
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            }
        )
        self.base_url = ""
        self.articles = []
        self.news_index = 0

    def scrape(self):
        """å­ç±»éœ€å®ç°çš„å…·ä½“çˆ¬å–é€»è¾‘"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° scrape æ–¹æ³•")

    def get_random_delay(self):
        """éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°"""
        time.sleep(random.uniform(1, 3))

    def calculate_weight(self, title):
        """è®¡ç®—æ–°é—»æƒé‡"""
        return self.calculate_keyword_weights([title], KEYWORDS_RANK_MAP)

    def determine_category(self, title, keywords):
        """æ ¹æ®æ ‡é¢˜å’Œå…³é”®è¯ç¡®å®šæ–‡ç« åˆ†ç±»"""
        title_lower = title.lower()

        # å®šä¹‰åˆ†ç±»è§„åˆ™
        categories = {
            "äººå·¥æ™ºèƒ½": [
                "ai",
                "intelligence",
                "learning",
                "neural",
                "gpt",
                "openai",
                "transformer",
                "robot",
            ],
            "äº§å“ç±»": [
                "product",
                "tesla",
                "apple",
                "google",
                "amazon",
                "chip",
                "device",
                "smart",
            ],
            "å†›å·¥": [
                "missile",
                "warship",
                "drone",
                "darpa",
                "defense",
                "military",
                "unmanned",
            ],
            "ç¤¾ä¼š": [
                "economic",
                "market",
                "work",
                "policy",
                "social",
                "climate",
                "sustainability",
            ],
            "ç§‘æŠ€": [],  # é»˜è®¤åˆ†ç±»
        }

        for cat, words in categories.items():
            if any(word in title_lower for word in words):
                return cat
        return "ç§‘æŠ€"

    def filter_and_store(self, keywords="ç§‘æŠ€"):
        """è¿‡æ»¤å¹¶å­˜å‚¨ç¬¦åˆæ¡ä»¶çš„æ–‡ç« """
        filtered_articles = []
        current_date = datetime.now()
        six_months_ago = current_date - timedelta(days=180)

        for title, url, weight in self.articles:
            if weight > kRankLevelValue:
                # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
                db_publish_time_str = mysqlWriteNewsV2.getArticlePublishTime(url)

                if db_publish_time_str:
                    try:
                        # å°è¯•è§£ææ•°æ®åº“ä¸­çš„æ—¶é—´æ ¼å¼ %Y-%m-%d_%H:%M
                        db_publish_time = datetime.strptime(
                            db_publish_time_str, "%Y-%m-%d_%H:%M"
                        )
                        if db_publish_time < six_months_ago:
                            print(f"æ–‡ç« å·²è¶…è¿‡åŠå¹´ï¼Œä¸å†å‘é€: {url}")
                            continue
                        else:
                            print(f"æ–‡ç« å·²åœ¨æ•°æ®åº“ä½†æœªæ»¡åŠå¹´ï¼Œç»§ç»­å‘é€: {title[:50]}...")
                            filtered_articles.append((title, url, weight))
                            continue
                    except Exception as e:
                        print(f"è§£ææ•°æ®åº“æ—¶é—´å¤±è´¥: {e}, é»˜è®¤è·³è¿‡")
                        continue

                filtered_articles.append((title, url, weight))

                # ç¡®å®šåˆ†ç±»
                category = self.determine_category(title, keywords)

                # å†™å…¥æ•°æ®åº“ (ç§»é™¤ Id å­—æ®µï¼Œè®©æ•°æ®åº“è‡ªå¢)
                publish_time = datetime.now().strftime("%Y-%m-%d_%H:%M")
                newsOne = (
                    weight,
                    title,
                    self.source_name,
                    publish_time,
                    "content",
                    url,
                    keywords,
                    category,
                )
                sql = """ INSERT INTO techTB(Rate,title,author,publish_time,content,url,key_word,category) VALUES(%s,%s,%s,%s,%s,%s,%s,%s) """
                result = mysqlWriteNewsV2.writeDb(sql, newsOne)
                if result:
                    print(f"âœ… æˆåŠŸå†™å…¥æ•°æ®åº“ [{category}]: {title[:50]}...")
                else:
                    print(f"âŒ å†™å…¥æ•°æ®åº“å¤±è´¥: {title[:50]}...")

        return filtered_articles

    def compute_rank_from_map(self, text, key_map, fuzzy=False, threshold=0.8):
        if not text or not key_map:
            return 0.0
        text_lower = re.sub(r"\W+", " ", text).lower()
        tokens = text_lower.split()
        rank = 0.0
        for key, weight in key_map.items():
            try:
                key_l = key.lower()
            except Exception:
                continue
            if key_l in text_lower:
                rank += float(weight)
            elif fuzzy:
                seq = difflib.SequenceMatcher(None, key_l, text_lower)
                if seq.ratio() >= threshold:
                    rank += float(weight) * seq.ratio()
                else:
                    for t in tokens:
                        seq2 = difflib.SequenceMatcher(None, key_l, t)
                        if seq2.ratio() >= threshold:
                            rank += float(weight) * seq2.ratio()
                            break
        return float(rank)

    # è®¡ç®—å…³é”®è¯æƒé‡
    def calculate_keyword_weights(self, texts, keywords):
        vectorizer = TfidfVectorizer()

        tfidf_matrix = vectorizer.fit_transform(texts)
        ##scikit-learn>1.0.x use this version
        feature_names = vectorizer.get_feature_names_out()
        print("test feature_names", feature_names)
        # Define a set of common words to filter out (stop words)
        # Filter out stop words and numbers from feature names
        filtered_feature_names = [
            feature
            for feature in feature_names
            if feature not in stop_words and not feature.isdigit()
        ]
        # Assuming we want to select the top N important feature names
        # For demonstration, let's say we want the top 3 features
        # You can replace this logic with your own importance criteria
        text = " ".join(texts)
        return self.compute_rank_from_map(text, keywords, fuzzy=True, threshold=0.7)
        # top_n = 6
        # important_feature_names = filtered_feature_names[:top_n]  # Select top N feature names
        # print("important_feature_names:{0}".format(important_feature_names));
        # keyword_indices = []
        # keyword_weights_sum = 0
        # for keyword in keywords:
        #     if keyword in filtered_feature_names:
        #         index = filtered_feature_names.index(keyword)
        #         keyword_indices.append(index)
        #         keyword_weights = tfidf_matrix[:, index].toarray()
        #         print("Keyword: {0}, Index: {1}, Weight: {2}".format(keyword, index, keyword_weights))
        #         keyword_weights_sum += keyword_weights.sum()
        # return keyword_weights_sum


class MitScraper(NewsScraper):
    """MIT Technology Review çˆ¬è™«"""

    def __init__(self):
        super().__init__("MIT Technology Review")

    def scrape(self):
        url = "https://www.technologyreview.com/"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")

        news_elements = soup.find_all(
            class_="homepageStoryCard__wrapper--5d95dc382241d259dc249996a6e29782"
        )
        print(f"weight test")
        for news_element in news_elements:
            try:
                title_elem = news_element.find(
                    class_="homepageStoryCard__hed--92c78a74bbc694463e43e32aafbbdfd7"
                )
                link_elem = news_element.find("a")

                if title_elem and link_elem:
                    title = title_elem.text.strip()
                    url = link_elem["href"]

                    # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                    if not url.startswith("http"):
                        url = f"https://www.technologyreview.com{url}"

                    # è®¡ç®—æ–°é—»æƒé‡
                    weight = self.calculate_weight(title)
                    if weight > 0:
                        print(f"weight is {weight}")
                        self.articles.append((title, url, weight))
            except Exception as e:
                print(f"å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {str(e)}")

        return self.filter_and_store("MITç§‘æŠ€è¯„è®º")


class HackerNewsScraper(NewsScraper):
    """Hacker News çˆ¬è™«"""

    def __init__(self):
        # super().__init__("Hacker News")
        super().__init__("Hacker News")
        self.base_url = "https://hacker-news.firebaseio.com/v0"

    def scrape(self, limit: int = 10) -> List[Dict]:
        """çˆ¬å– Hacker News çƒ­é—¨æ–‡ç« """
        articles = []
        try:
            # è·å–çƒ­é—¨æ–‡ç« ID
            response = self.session.get(f"{self.base_url}/topstories.json")
            story_ids = response.json()[:limit]

            for story_id in story_ids:
                self.get_random_delay()

                # è·å–æ–‡ç« è¯¦æƒ…
                story_response = self.session.get(
                    f"{self.base_url}/item/{story_id}.json"
                )
                story_data = story_response.json()

                if story_data and story_data.get("url"):
                    article = {
                        "title": story_data.get("title", ""),
                        "url": story_data.get("url", ""),
                        "summary": f"Hacker Newsçƒ­é—¨æ–‡ç« ï¼Œå¾—åˆ†ï¼š{story_data.get('score', 0)}",
                        "source": self.source_name,
                        "author": story_data.get("by", ""),
                        "tags": "Tech,News",
                        "publish_time": datetime.fromtimestamp(
                            story_data.get("time", 0)
                        ),
                        "views": story_data.get("score", 0),
                        "likes": story_data.get("descendants", 0),
                    }
                    # è®¡ç®—æ–°é—»æƒé‡
                    weight = self.calculate_weight(article["title"])
                    self.articles.append((article["title"], article["url"], weight))

            print(f"æˆåŠŸçˆ¬å– {len(self.articles)} ç¯‡ Hacker News æ–‡ç« ")
            return self.filter_and_store("Hacker News")

        except Exception as e:
            print(f"çˆ¬å– Hacker News å¤±è´¥: {e}")
            return []


class GitHubTrendingScraper(NewsScraper):
    """GitHub Trending çˆ¬è™«åŒ…è£…å™¨"""

    def __init__(self):
        super().__init__("GitHub Trending")
        self.scraper = scrapers.GitHubTrendingScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("GitHub")
        except Exception as e:
            print(f"GitHub çˆ¬å–å¤±è´¥: {e}")
            return []


class RedditScraper(NewsScraper):
    """Reddit çˆ¬è™«åŒ…è£…å™¨"""

    def __init__(self):
        super().__init__("Reddit Programming")
        self.scraper = scrapers.RedditScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("Reddit")
        except Exception as e:
            print(f"Reddit çˆ¬å–å¤±è´¥: {e}")
            return []


class DevToScraper(NewsScraper):
    """Dev.to çˆ¬è™«åŒ…è£…å™¨"""

    def __init__(self):
        super().__init__("Dev.to")
        self.scraper = scrapers.DevToScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("Dev.to")
        except Exception as e:
            print(f"Dev.to çˆ¬å–å¤±è´¥: {e}")
            return []


class AITopicsScraper(NewsScraper):
    """AI Topics çˆ¬è™«åŒ…è£…å™¨"""

    def __init__(self):
        super().__init__("AI Topics")
        self.scraper = scrapers.AITopicsScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("äººå·¥æ™ºèƒ½")
        except Exception as e:
            print(f"AI Topics çˆ¬å–å¤±è´¥: {e}")
            return []


class MediumScraper(NewsScraper):
    """Medium çˆ¬è™«åŒ…è£…å™¨"""

    def __init__(self):
        super().__init__("Medium Technology")
        self.scraper = scrapers.MediumScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("Medium")
        except Exception as e:
            print(f"Medium çˆ¬å–å¤±è´¥: {e}")
            return []


class TechCrunchScraper(NewsScraper):
    """TechCrunch çˆ¬è™«åŒ…è£…å™¨"""

    def __init__(self):
        super().__init__("TechCrunch")
        self.scraper = scrapers.TechCrunchScraper()

    def scrape(self, limit=10):
        try:
            articles = self.scraper.scrape_articles(limit=limit)
            for art in articles:
                weight = self.calculate_weight(art["title"])
                self.articles.append((art["title"], art["url"], weight))
            return self.filter_and_store("TechCrunch")
        except Exception as e:
            print(f"TechCrunch çˆ¬å–å¤±è´¥: {e}")
            return []


class TechNewsAggregator:
    """ç§‘æŠ€æ–°é—»èšåˆå™¨"""

    def __init__(self):
        self.scrapers = [
            MitScraper(),
            GitHubTrendingScraper(),
            AITopicsScraper(),
            DevToScraper(),
            RedditScraper(),
            HackerNewsScraper(),
            MediumScraper() if "MediumScraper" in globals() else None,
            TechCrunchScraper() if "TechCrunchScraper" in globals() else None,
        ]
        # è¿‡æ»¤æ‰æœªå®šä¹‰çš„çˆ¬è™«
        self.scrapers = [s for s in self.scrapers if s is not None]

    def collect_news(self):
        """æ”¶é›†æ‰€æœ‰æ¥æºçš„æ–°é—»"""
        all_articles = []

        print(f"ğŸ“… å¼€å§‹æ”¶é›†ç§‘æŠ€æ–°é—» - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ” ä»·å€¼é˜ˆå€¼: > {kRankLevelValue}")
        print("-" * 60)

        for scraper in self.scrapers:
            print(f"\nğŸ“¡ æ­£åœ¨é‡‡é›† {scraper.source_name}...")
            try:
                articles = scraper.scrape()
                print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æœ‰ä»·å€¼æ–‡ç« ")
                all_articles.extend(articles)
            except Exception as e:
                print(f"âŒ é‡‡é›†å¤±è´¥: {str(e)}")

        # æŒ‰æƒé‡æ’åº
        all_articles.sort(key=lambda x: x[2], reverse=True)
        return all_articles

    def save_to_txt(self, articles, filename=OUTPUT_FILE):
        """å°†æ–°é—»ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶"""
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"æ¯æ—¥ç§‘æŠ€æ–°é—»æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d')}\n")
            f.write(f"å…±æ”¶é›†åˆ° {len(articles)} ç¯‡é«˜ä»·å€¼æ–‡ç« \n")
            f.write("=" * 60 + "\n\n")

            for idx, (title, url, weight) in enumerate(articles, 1):
                f.write(f"{idx}. [{weight:.2f}] {title}\n")
                f.write(f"   ğŸ”— {url}\n\n")

        print(f"ğŸ’¾ æ–°é—»å·²ä¿å­˜åˆ° {os.path.abspath(filename)}")
        return filename


def send_news_email(txt_file, recipient):
    """å‘é€åŒ…å«æ–°é—»æ‘˜è¦çš„é‚®ä»¶"""
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    import smtplib

    _pwd = encrypt_and_verify_url.decrypt_getKey(
        "dm1wbmFmYmxsdnR0YmJlXX==".encode("utf-8")
    )
    # è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹
    with open(txt_file, "r", encoding="utf-8") as f:
        news_content = f.read()

    # é‚®ä»¶é…ç½®
    sender = "840056598@qq.com"
    password = _pwd
    subject = f"æ¯æ—¥ç§‘æŠ€æ–°é—»æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d')}"

    # åˆ›å»ºé‚®ä»¶
    msg = MIMEMultipart()
    msg["From"] = sender
    receiver = recipient
    msg["To"] = formataddr(["äº²çˆ±çš„ç”¨æˆ·", receiver])  # æ‹¬å·é‡Œçš„å¯¹åº”æ”¶ä»¶äººé‚®ç®±
    msg["Subject"] = subject

    # æ·»åŠ æ–‡æœ¬å†…å®¹
    msg.attach(MIMEText(news_content, "plain", "utf-8"))
    print(f"send subject {subject}")
    # åˆ›å»ºå®‰å…¨ä¸Šä¸‹æ–‡ï¼ˆè§£å†³SSLéªŒè¯é—®é¢˜ï¼‰
    context = ssl.create_default_context()
    context.check_hostname = False
    context.verify_mode = ssl.CERT_NONE
    # å‘é€é‚®ä»¶
    try:
        # with smtplib.SMTP_SSL('smtp.qq.com', 465, context) as server:
        # server.login(sender, _pwd.decode("utf-8"))
        # print("login email OK\n")
        # server.sendmail(sender, [receiver,], msg.as_string())
        server = smtplib.SMTP_SSL(
            "smtp.qq.com", 465
        )  # å‘ä»¶äººé‚®ç®±ä¸­çš„SMTPæœåŠ¡å™¨ï¼Œç«¯å£æ˜¯25 (é»˜è®¤ï¼‰---------->465
        server.login(sender, _pwd.decode("utf-8"))  # æ‹¬å·ä¸­å¯¹åº”çš„æ˜¯å‘ä»¶äººé‚®ç®±è´¦å·ã€é‚®ç®±å¯†ç 
        server.sendmail(
            sender,
            [
                receiver,
            ],
            msg.as_string(),
        )  # æ‹¬å·ä¸­å¯¹åº”çš„æ˜¯å‘ä»¶äººé‚®ç®±è´¦å·ã€æ”¶ä»¶äººé‚®ç®±è´¦å·ã€å‘é€é‚®ä»¶
        print(f"ğŸ“§ é‚®ä»¶å·²æˆåŠŸå‘é€è‡³ {recipient}")
        print("SEND NEWS AND IMG OK")
        server.quit()  # è¿™å¥æ˜¯å…³é—­è¿æ¥
        return True
    except smtplib.SMTPAuthenticationError:
        print("âŒ è®¤è¯å¤±è´¥: è¯·æ£€æŸ¥é‚®ç®±å’Œæˆæƒç æ˜¯å¦æ­£ç¡®")
        print("ğŸ’¡ æç¤º: QQé‚®ç®±éœ€è¦ä½¿ç”¨æˆæƒç è€Œéå¯†ç ")
    except smtplib.SMTPException as e:
        print(f"âŒ SMTPåè®®é”™è¯¯: {str(e)}")
        print(f"é”™è¯¯ä»£ç : {e.smtp_code}")
        print(f"é”™è¯¯æ¶ˆæ¯: {e.smtp_error.decode('utf-8')}")
    except Exception as e:
        print(f"âŒ å‘é€å¤±è´¥: {str(e)}")


# ä¸»æ‰§è¡Œæµç¨‹
if __name__ == "__main__":
    with open("./tech_key_config_map.json") as j:

        KEYWORDS_RANK_MAP = json.load(j)["KEYWORDS_RANK_MAP"]

    # åˆ›å»ºèšåˆå™¨å¹¶æ”¶é›†æ–°é—»
    aggregator = TechNewsAggregator()
    articles = aggregator.collect_news()

    # ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
    txt_file = aggregator.save_to_txt(articles)

    # å‘é€é‚®ä»¶ï¼ˆé™¤é dry-runï¼‰
    # if args.dry_run:
    # print("--dry-run: è·³è¿‡å‘é€é‚®ä»¶")
    # else:
    send_news_email(txt_file, "840056598@qq.com")

    # å¯é€‰ï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶
    # os.remove(txt_file)

