#!/usr/bin/env python3
"""
TechDaily æŠ€æœ¯æ–‡ç« é‡‡é›†å™¨
è‡ªåŠ¨ä»å…¨çƒçŸ¥åITæŠ€æœ¯ç½‘ç«™é‡‡é›†æœ€æ–°æ–‡ç« 
"""

import sys
import os
import argparse
from datetime import datetime
from typing import List

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from database import TechNewsDB
from scrapers import (
    HackerNewsScraper,
    RedditScraper, 
    GitHubTrendingScraper,
    DevToScraper,
    MediumScraper,
    TechCrunchScraper
)

class TechCrawler:
    """æŠ€æœ¯æ–‡ç« çˆ¬è™«ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, db_path="tech_news.db"):
        self.db = TechNewsDB(db_path)
        self.scrapers = {
            'hackernews': HackerNewsScraper(),
            'reddit': RedditScraper(),
            'github': GitHubTrendingScraper(),
            'devto': DevToScraper(),
            'medium': MediumScraper(),
            'techcrunch': TechCrunchScraper()
        }
        
    def crawl_all_sources(self, articles_per_source=2):
        """ä»æ‰€æœ‰æºé‡‡é›†æ–‡ç« """
        total_articles = 0
        total_success = 0
        
        print(f"ğŸš€ å¼€å§‹é‡‡é›†å…¨çƒITæŠ€æœ¯æ–‡ç« ...")
        print(f"ğŸ“… é‡‡é›†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ ç›®æ ‡: æ¯ä¸ªæºé‡‡é›† {articles_per_source} ç¯‡æ–‡ç« ")
        print("-" * 60)
        
        for source_name, scraper in self.scrapers.items():
            print(f"\nğŸ“¡ æ­£åœ¨é‡‡é›† {scraper.source}...")
            
            try:
                articles = scraper.scrape_articles(limit=articles_per_source)
                success_count = 0
                error_count = 0
                
                for article in articles:
                    article_id = self.db.insert_article(article)
                    if article_id:
                        success_count += 1
                        total_success += 1
                    else:
                        error_count += 1
                    
                    total_articles += 1
                
                # è®°å½•çˆ¬å–ç»Ÿè®¡
                self.db.insert_crawl_record(
                    source=scraper.source,
                    total=len(articles),
                    success=success_count,
                    error=error_count,
                    status="completed"
                )
                
                print(f"âœ… {scraper.source}: é‡‡é›† {len(articles)} ç¯‡ï¼ŒæˆåŠŸ {success_count} ç¯‡ï¼Œé‡å¤ {error_count} ç¯‡")
                
            except Exception as e:
                print(f"âŒ {scraper.source} é‡‡é›†å¤±è´¥: {e}")
                self.db.insert_crawl_record(
                    source=scraper.source,
                    total=0,
                    success=0,
                    error=1,
                    status="failed",
                    error_msg=str(e)
                )
        
        print("\n" + "=" * 60)
        print(f"ğŸ‰ é‡‡é›†å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡å°è¯•é‡‡é›†: {total_articles} ç¯‡")
        print(f"âœ… æˆåŠŸå­˜å‚¨: {total_success} ç¯‡")
        print(f"ğŸ”„ é‡å¤æ–‡ç« : {total_articles - total_success} ç¯‡")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        self.show_statistics()
        
    def crawl_single_source(self, source_name, limit=10):
        """ä»å•ä¸ªæºé‡‡é›†æ–‡ç« """
        if source_name not in self.scrapers:
            print(f"âŒ æœªçŸ¥çš„æ•°æ®æº: {source_name}")
            print(f"ğŸ“‹ å¯ç”¨çš„æ•°æ®æº: {', '.join(self.scrapers.keys())}")
            return
        
        scraper = self.scrapers[source_name]
        print(f"ğŸ“¡ æ­£åœ¨ä» {scraper.source} é‡‡é›† {limit} ç¯‡æ–‡ç« ...")
        
        try:
            articles = scraper.scrape_articles(limit=limit)
            success_count = 0
            
            for article in articles:
                article_id = self.db.insert_article(article)
                if article_id:
                    success_count += 1
            
            self.db.insert_crawl_record(
                source=scraper.source,
                total=len(articles),
                success=success_count,
                error=len(articles) - success_count,
                status="completed"
            )
            
            print(f"âœ… é‡‡é›†å®Œæˆ: {len(articles)} ç¯‡ï¼ŒæˆåŠŸå­˜å‚¨ {success_count} ç¯‡")
            
        except Exception as e:
            print(f"âŒ é‡‡é›†å¤±è´¥: {e}")
    
    def show_statistics(self):
        """æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.db.get_crawl_stats()
        
        print(f"\nğŸ“ˆ æ•°æ®åº“ç»Ÿè®¡:")
        print(f"ğŸ“š æ€»æ–‡ç« æ•°: {stats.get('total_articles', 0)}")
        print(f"ğŸ“… ä»Šæ—¥é‡‡é›†: {stats.get('today_articles', 0)}")
        
        print(f"\nğŸ“Š æŒ‰æ¥æºç»Ÿè®¡:")
        for source, count in stats.get('source_stats', []):
            print(f"  {source}: {count} ç¯‡")
    
    def list_recent_articles(self, limit=10):
        """åˆ—å‡ºæœ€è¿‘é‡‡é›†çš„æ–‡ç« """
        articles = self.db.get_articles(limit=limit)
        
        print(f"\nğŸ“° æœ€è¿‘é‡‡é›†çš„ {len(articles)} ç¯‡æ–‡ç« :")
        print("-" * 80)
        
        for i, article in enumerate(articles, 1):
            print(f"{i}. ã€{article['source']}ã€‘{article['title'][:60]}...")
            print(f"   ğŸ”— {article['url']}")
            print(f"   â° {article['crawl_time']}")
            print()

def main():
    parser = argparse.ArgumentParser(description='TechDaily æŠ€æœ¯æ–‡ç« é‡‡é›†å™¨')
    #parser.add_argument('--source', help='æŒ‡å®šé‡‡é›†æº (hackernews, reddit, github, devto, medium, techcrunch)')
    parser.add_argument('--source', help='æŒ‡å®šé‡‡é›†æº (reddit, github, devto, medium)')
    
    parser.add_argument('--limit', type=int, default=10, help='æ¯ä¸ªæºé‡‡é›†çš„æ–‡ç« æ•°é‡ (é»˜è®¤: 10)')
    parser.add_argument('--list', action='store_true', help='åˆ—å‡ºæœ€è¿‘é‡‡é›†çš„æ–‡ç« ')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿé‡‡é›†æ¨¡å¼ (æ¯ä¸ªæº2ç¯‡)')
    
    args = parser.parse_args()
    
    crawler = TechCrawler()
    
    if args.list:
        crawler.list_recent_articles(args.limit)
    elif args.stats:
        crawler.show_statistics()
    elif args.source:
        crawler.crawl_single_source(args.source, args.limit)
    elif args.quick:
        crawler.crawl_all_sources(articles_per_source=2)
    else:
        # é»˜è®¤é‡‡é›†æ¨¡å¼ï¼šæ¯ä¸ªæºé‡‡é›†æŒ‡å®šæ•°é‡çš„æ–‡ç« 
        articles_per_source = min(args.limit, 3)  # é™åˆ¶æ¯ä¸ªæºæœ€å¤š3ç¯‡ï¼Œé¿å…è¿‡è½½
        crawler.crawl_all_sources(articles_per_source)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  é‡‡é›†è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc() 
